Import libraries
"""

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split, GridSearchCV
import nltk
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.corpus import wordnet

"""Dowloading the required NLTK packages :



1.   wordnet : downloads the WordNet lexical database
2.   punkt : downloads the Punkt tokenizer models, which are used for     
     splitting text into words or sentences
3.   averaged_perceptron_tagger : downloads a part-of-speech tagger
4.   omw-1.4 : downloads the Open Multilingual WordNet data


"""

nltk.download("wordnet")
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')

# Load dataset
df = pd.read_csv('bbc_text_cls.csv')

df.head()

len(df)

"""Identifying the inputs and respective lables"""

# Identify inputs and labels
inputs = df['text']
labels = df['labels']

# Visualizing label distribution
labels.hist(figsize=(10, 5));

"""Splitting the dataset to train and test dataset (default : 75% - 25% split)"""

# Split dataset into train and test sets (75% - 25%)
inputs_train, inputs_test, Ytrain, Ytest = train_test_split(inputs, labels, random_state=123)

vectorizer = CountVectorizer()

Xtrain = vectorizer.fit_transform(inputs_train)
Xtest = vectorizer.transform(inputs_test)

type(Xtrain)

# Convert sparse matrix to dense representation
dense_matrix = Xtrain.toarray()
print(dense_matrix)

"""Counting the number of non-zero values"""

# Count non-zero values
(Xtrain != 0).sum()

"""Finding the proportion of non-zero values wrt the total number of elements in the dataset"""

# Proportion of non-zero values
(Xtrain != 0).sum() / np.prod(Xtrain.shape)

"""Multinomial Naive Bayes is a classification model that uses the word counts (or other frequency-based features) generated by count vectorization method, to calculate the probability of each class and classify the data accordingly."""

model = MultinomialNB()
model.fit(Xtrain, Ytrain)
print("train score:", model.score(Xtrain, Ytrain))
print("test score:", model.score(Xtest, Ytest))

"""The above train and test score is obtained where the model gives predictions, by not excluding stop words

In the below cell, we are explicitly removing the stop words and recalculating the accuracy.
"""

# with stopwords
vectorizer = CountVectorizer(stop_words='english')
Xtrain = vectorizer.fit_transform(inputs_train)
Xtest = vectorizer.transform(inputs_test)
model = MultinomialNB()
model.fit(Xtrain, Ytrain)
print("train score:", model.score(Xtrain, Ytrain))
print("test score:", model.score(Xtest, Ytest))

"""The accuracy scores have slightly increased after removing the stop words.

Using the tree bank dataset, to identify the parts of speech
"""

# --- Lemmatization Using Custom Tokenizer ---
def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

class LemmaTokenizer:
    def __init__(self):
        self.wnl = WordNetLemmatizer()
    def __call__(self, doc):
        tokens = word_tokenize(doc)
        words_and_tags = nltk.pos_tag(tokens)
        return [self.wnl.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in words_and_tags]

nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')

# with lemmatization
vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())
Xtrain = vectorizer.fit_transform(inputs_train)
Xtest = vectorizer.transform(inputs_test)
model = MultinomialNB()
model.fit(Xtrain, Ytrain)
print("train score:", model.score(Xtrain, Ytrain))
print("test score:", model.score(Xtest, Ytest))

# --- Stemming Using Custom Tokenizer ---
class StemTokenizer:
    def __init__(self):
        self.porter = PorterStemmer()
    def __call__(self, doc):
        tokens = word_tokenize(doc)
        return [self.porter.stem(t) for t in tokens]

# with stemming
vectorizer = CountVectorizer(tokenizer=StemTokenizer())
Xtrain = vectorizer.fit_transform(inputs_train)
Xtest = vectorizer.transform(inputs_test)
model = MultinomialNB()
model.fit(Xtrain, Ytrain)
print("train score:", model.score(Xtrain, Ytrain))
print("test score:", model.score(Xtest, Ytest))

# --- Simple String Split Tokenizer ---
def simple_tokenizer(s):
    return s.split()

# string split tokenizer
vectorizer = CountVectorizer(tokenizer=simple_tokenizer)
Xtrain = vectorizer.fit_transform(inputs_train)
Xtest = vectorizer.transform(inputs_test)
model = MultinomialNB()
model.fit(Xtrain, Ytrain)
print("train score:", model.score(Xtrain, Ytrain))
print("test score:", model.score(Xtest, Ytest))

"""

---

Extra Enhancements

---

"""

# 1. **TF-IDF Instead of CountVectorizer**
vectorizer_tfidf = TfidfVectorizer()
Xtrain_tfidf = vectorizer_tfidf.fit_transform(inputs_train)
Xtest_tfidf = vectorizer_tfidf.transform(inputs_test)
model_tfidf = MultinomialNB()
model_tfidf.fit(Xtrain_tfidf, Ytrain)
print("TF-IDF train score:", model_tfidf.score(Xtrain_tfidf, Ytrain))
print("TF-IDF test score:", model_tfidf.score(Xtest_tfidf, Ytest))

# 2. **Hyperparameter Tuning Using GridSearchCV**
param_grid = {'alpha': [0.1, 0.5, 1.0, 1.5, 2.0]}
grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5)
grid_search.fit(Xtrain, Ytrain)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

# 3. **Handling Out-of-Vocabulary Words**
vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), max_features=5000)  # Limit vocabulary size
Xtrain = vectorizer.fit_transform(inputs_train)
Xtest = vectorizer.transform(inputs_test)
model = MultinomialNB()
model.fit(Xtrain, Ytrain)
print("train score (limited vocab):", model.score(Xtrain, Ytrain))
print("test score (limited vocab):", model.score(Xtest, Ytest))
